\documentclass{article}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{color}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\expected}{\mathbb{E}}
\definecolor{bostonuniversityred}{rgb}{0.8, 0.0, 0.0}

\begin{document}
	\section{}
	\paragraph{MAML}
	\begin{algorithm}
		\caption{MAML}
		\begin{algorithmic}  
			\STATE Randomly initialize \textcolor{bostonuniversityred}{ $\boldsymbol{\mu}_{\theta}, \boldsymbol{\sigma}^2_{\theta}$}
			\WHILE{not done}
			\FOR{task $\mathcal{T}_i \sim p(\mathcal{T})$}
			\STATE Draw support set $\mathcal{D}_i^{S} = \{ (\boldsymbol{x}_j, \boldsymbol{y}_j)\}_{j=1...K}$ from $\mathcal{T}_i$
			\STATE \textcolor{bostonuniversityred}{ sample $\theta \sim \mathcal{N}(\boldsymbol{\mu}_{\theta}, \boldsymbol{\sigma}^2_{\theta})$}
			\STATE Adapt parameters $\theta_i = \theta - \alpha \nabla_{\theta} \mathcal{L}_i(\theta, \mathcal{D}_i^{S}) $
			\STATE Draw test samples  $\mathcal{D}_i^{Q} = \{(\boldsymbol{x}_j, \boldsymbol{y}_j)\}$ from $\mathcal{T}_i$
			\ENDFOR
			\STATE Meta-Update: $\textcolor{bostonuniversityred}{\boldsymbol{\mu}_\theta} \leftarrow \textcolor{bostonuniversityred}{\boldsymbol{\mu}_\theta} - \beta \nabla_{\textcolor{bostonuniversityred}{\boldsymbol{\mu}_\theta}} \sum_{i} \mathcal{L}_i(\theta_i, \mathcal{D}_{i}^{Q})$
			\STATE Meta-Update: $\textcolor{bostonuniversityred}{\boldsymbol{\sigma}^2_\theta} \leftarrow \textcolor{bostonuniversityred}{\boldsymbol{\sigma}^2_\theta} - \beta \nabla_{\textcolor{bostonuniversityred}{\boldsymbol{\sigma}^2_\theta}} \sum_{i} \mathcal{L}_i(\theta_i, \mathcal{D}_{i}^{Q})$
			\ENDWHILE
		\end{algorithmic}
	\end{algorithm}
	\section{graphical models}
		\begin{align*}
			&p(\boldsymbol{\theta}, \boldsymbol{\phi}_{1:T}, \boldsymbol{x}_{1:T, 1:N+M}, \boldsymbol{y}_{1:T, 1:N+M}) \\
			&= \left( \prod_{i=1}^{T} \left( \prod_{j=1}^{N+M} p(\boldsymbol{x}_{1:T, 1:N+M}, \boldsymbol{y}_{1:T, 1:N+M} \vert \phi_i) \right) p(\phi_i \vert \theta) \right) p(\theta)
		\end{align*}
	What does it mean to have observed data? In Meta Training we basically have given all samples. In meta testing we dont have the test sample targets. Classical MAML:
	\begin{align*}
		p(y_i^{test} \vert x_i^{test}, x_i^{tr}, y_i^{tr}) = \int p(y_i^{test} \vert x_i^{test}, \phi_i) p(\phi_i \vert x_i^{tr}, y_i^{tr}) d \phi_i
	\end{align*}
\newpage
	\section{first lower bound equation}
	General Variational Bayes:
	\begin{align}
		\text{log } p(x) = \expected_{z \sim q}[\text{log } \dfrac{p(x \vert z)p(z)}{q(z)}] + KL \big(q(z) \Vert p(z \vert x)\big)
	\end{align}
	Mapping from general VB to the Probabilistic MAML case:
	\begin{align}
		p(x) &= p(y_i^{test} \vert x_i^{tr, test}, y_i^{tr}) \\
		p(z) &= p(\phi_i, \theta \vert x_i^{tr,test}, y_i^{tr}) \\
		p(x \vert z) &= p(y_i^{test} \vert \phi_i, x_i^{tr, test}, y_i^{tr})\\
		q(z) &= q_{\psi}(\phi_i \vert \theta, x_i^{tr,test}, y_i^{tr,test}) q_{\psi}(\theta \vert x_i^{tr,test}, y_i^{tr,test})
	\end{align}
	Lower Bound for Probabilistic MAML:
	\begin{align}
		&\text{log } p(y_i^{test} \vert x_i^{tr, test}, y_i^{tr}) \\ 
		&\ge  \expected_{\theta, \phi_i \sim q_{\psi}}[\text{log } p(y_i^{test} \vert \phi_i, x_i^{tr, test}, y_i^{tr}) + \text{log }p(\phi_i, \theta \vert x_i^{tr,test}, y_i^{tr})] \\
		&- \expected_{\theta, \phi_i \sim q_{\psi}}[  \text{log }q_{\psi}(\phi_i \vert \theta, x_i^{tr,test}, y_i^{tr,test}) q_{\psi}(\theta \vert x_i^{tr,test}, y_i^{tr,test})]  \\
		&= \expected_{\theta, \phi_i \sim q_{\psi}}[ \text{log } p(y_i^{test} \vert \phi_i, x_i^{test}) + \textcolor{bostonuniversityred}{ \text{log }p(\phi_i, \theta, y_i^{tr} \vert x_i^{tr}) - \text{log } p(y_i^{tr} \vert x_i^{tr})}] \\
		&+ \mathcal{H}(q_{\psi}(\phi_i \vert \theta, x_i^{tr,test}, y_i^{tr,test})) + \mathcal{H}( q_{\psi}(\theta \vert x_i^{tr,test}, y_i^{tr,test})) 
	\end{align}
	Only the red part:
	\begin{align}
		\textcolor{bostonuniversityred}{\text{log } p(y_i^{tr} \vert x_i^{tr})} &= \text{const} \\
		\textcolor{bostonuniversityred}{\text{log }p(\phi_i, \theta, y_i^{tr} \vert x_i^{tr})} &= \text{log } p(y_i^{tr} \vert x_i^{tr}, \phi_i, \theta)p(\phi_i, \theta \vert x_i^{tr}) \\
		&= \text{log } p(y_i^{tr} \vert x_i^{tr}, \phi_i) + \text{log }p(\phi_i \vert \theta) + \text{log } p(\theta)
	\end{align}
	
	\section{}
	\subsection{Gradient-based Meta Learning with variational Inference}
	In LLAMA we used deterministic $p(\theta)$ and did gradient descent on $log(y^{tr} \vert x^{tr}, \theta)$ to get the next $\phi_{k+1}$. Now we use structured variational inference to approximate $p(\phi_i, \theta)$ by $q_i(\phi_i \vert \theta)q_i(\theta)$.We can avoid storing two distributions for each task by parameterizing one distribution family. 
	\begin{align}
		p(\phi_i, \theta) \approx q_{\psi}(\phi_i \vert \theta, x_i^{\text{tr,test}}, y_i^{\text{tr,test}}) q_{\psi}(\theta \vert x_i^{\text{tr,test}}, y_i^{\text{tr,test}})
	\end{align}
	We set up variational inference lower bound we want to maximize. We choose prior $p(\theta)= \mathcal{N}(\mu_{\theta}, \sigma_{\theta}^2)$ and $p(\phi_i \vert \theta)=\mathcal{N}(\theta, \sigma_{?}^2)$. For the inference networks we choose 
	\begin{align}
		q_{\psi}(\phi_i \vert \theta, x_i^{\text{tr,test}}, y_i^{\text{tr,test}}) = \mathcal{N}\Big( \mu_{\theta} + \gamma_{q} \nabla_{\mu_{\theta}} \text{log } p(y_i^{\text{tr,test}} \vert x_i^{tr,test}, \mu_{\theta}), v_q \Big)
	\end{align}
	for the other inference network this can be done as well, but only for meta-training. in meta-testing we don't have $y_i^{\text{test}}$ so we need to do something different.
	\paragraph{Overview} According to the left model we have:
	\begin{align}
		\phi_i \sim p(\phi_i \vert x_i^{\text{tr}}, y_i^{\text{tr}}) \propto \int p(y_i^{\text{tr}} \vert x_i^{\text{tr}}, \phi_i) p(\phi_i \vert \theta) p(\theta) d\theta
	\end{align}
	which is totally intractable, so we use point approximation of $\phi$ in the next section.
	
	\section{Probabilistic MAML with Hybrid Inference}
	we use maml to compute $p(\phi_i \vert x_i^{tr}, y_i^{tr}, \theta) \approx \delta(\phi - \phi^*)$ where $\phi^*$ is obtained by gradient descent over $\text{log } p(y_i^{tr} \vert x_i^{tr}, \theta)$. Again we define Lower Bound, but now only over $\theta$ as $\phi^*$ is now deterministic. 
	\begin{align}
		&\text{log }(y_i^{\text{test}} \vert x_i^{\text{tr+test}}, y_i^{\text{tr}}) \ge \text{Lower Bound}(\psi)\\
		&= E_{\theta \sim q_{\psi}}[\text{log } p(y_i^{\text{test}} \vert x_i^{\text{test}}, \phi^*) + \text{log } p(\theta)] + H(q_{\psi}(\theta \vert x_i^{\text{test}}, y_i^{\text{test}})) \\
		&= E_{\theta \sim q_{\psi}}[\text{log } p(y_i^{\text{test}} \vert x_i^{\text{test}}, \phi^*)] - KL(q_{\psi}(\theta \vert x_i^{\text{test}}, y_i^{\text{test}}) \Vert p(\theta))
	\end{align}
	In the end $q_{\psi}(\theta \vert x_i^{\text{test}}, y_i^{\text{test}})$ approximates $p(\theta \vert x_i^{\text{tr,test}}, y_i^{\text{tr,test}})$.

\end{document}